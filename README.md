# yapayzeka_masterclass

B. Aytan ve CO Sakar, "Türkçe ve Farklı Dillerde Eğitilen Transformatör Tabanlı Modellerin Türkçe Doğal Dil İşleme Sorunlarında Karşılaştırılması", 2022 30. Sinyal İşleme ve İletişim Uygulamaları Konferansı (SIU) , Safranbolu, Türkiye, 2022, s. 1- 4, doi: 10.1109/SIU55565.2022.9864818.


Makalesinin özeti;

Transformatör tabanlı önceden eğitilmiş dil modelleri, son yıllarda doğal dil işleme (NLP) problemlerinde başarılı sonuçlar vermektedir. Bu yaklaşımlarda modeller, maskeleme ve sonraki cümle tahmini gibi mekanizmalar kullanılarak, geniş bir derlem kullanılarak denetimsiz bir şekilde eğitilir. NLP altı problemlerde bu modeller ince ayar yaklaşımıyla tamamen veya kısmen güncellenir veya bu modellerden elde edilen vektörler sinir ağı katmanları eklenerek doğrudan çıktıya eşlenir. Bu çalışmada literatürde farklı dil ve problemlere yönelik başarılı sonuçlar veren transformatör tabanlı BERT, RoBERTa, ConvBERT ve Electra modelleri Türkçe duygu analizi, metin sınıflandırma ve adlandırılmış varlık tanıma problemlerine uygulanmış ve sonuçlar ortaya konulmuştur. karşılaştırmalı olarak sunulmuştur. Çalışmanın katkılarından biri de RoBERTa modelinin NLP alanındaki Türkçe problemlerinde kullanılmak üzere açık kaynak olarak paylaşılmak üzere toplam 38 GB Türkçe derlemi üzerinde eğitilmesidir. Deneysel sonuçlar, RoBERTaTurk modelinin Türk NLP problemlerinde diğer transformatör tabanlı modellerle karşılaştırılabilir sonuçlar verdiğini göstermiştir. Ayrıca farklı dillerde eğitilen dil modelleri de Türkçe sınıflandırma problemleri üzerinde test edilmiştir. Elde edilen sonuçlara göre etiketli eğitim setinin boyutu arttıkça farklı dillerde eğitilen dil modellerinin başarısı Türkçe için eğitilen dil modellerinin başarısına yaklaşmaktadır. Ayrıca etiketli veri sayısı az olduğunda Türkçe dil modellerinin daha iyi sonuçlar verdiği de gösterilmiştir.


E. Kanca, S. Ayas, EB Kablan ve M. Ekinci, "Tıbbi Görüntü Sınıflandırmada Görüş Transformatörü Tabanlı Modellerin Performans Karşılaştırması", 2023 31. Sinyal İşleme ve İletişim Uygulamaları Konferansı (SIU) , İstanbul, Türkiye, 2023, s. 1-4, doi: 10.1109/SIU59756.2023.10223892.

Makalesinin özeti;

Son yıllarda evrişimli sinir ağları önemli bir başarı göstermiş ve tıbbi görüntü analizi uygulamalarında sıklıkla kullanılmaktadır. Bununla birlikte, evrişimli sinir ağlarındaki evrişim süreci, yerel alıcı alandaki uzun vadeli piksel bağımlılıklarının öğrenilmesini sınırlar. Transformatör mimarilerinin uzun vadeli bağımlılıkları kodlama ve doğal dil işlemede daha verimli özellik temsilini öğrenmedeki başarısından ilham alınarak, kamuya açık renkli fundus retinası, cilt lezyonu, göğüs röntgeni ve meme histolojisi görüntüleri, Vision Transformer (ViT) kullanılarak sınıflandırılır. Bu çalışmada Veri Verimli Transformer (DeiT), Swin Transformer ve Pyramid Vision Transformer v2 (PVTv2) modelleri ve sınıflandırma performansları karşılaştırılmıştır. Sonuçlar en yüksek doğruluk değerlerinin 96'da DeiT modeliyle elde edildiğini göstermektedir. 


![image](https://github.com/miracelikbas/yapayzeka_masterclass/assets/74983047/bbad569f-d504-4714-8cef-bbcc74b96309)
